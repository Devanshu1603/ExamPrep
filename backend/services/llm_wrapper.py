# backend/services/llm_wrapper.py

import google.generativeai as genai
from config.env import GEMINI_API_KEY

genai.configure(api_key=GEMINI_API_KEY)

model = genai.GenerativeModel("gemini-2.0-flash")

def get_llm_response(prompt: str) -> str:
    try:
        print("\nğŸ”¹ Sending prompt to Gemini LLM...")
        print("ğŸ”¸ Prompt preview:\n", prompt[:500], "\n...")

        response = model.generate_content(prompt)

        print("ğŸ”¸ Raw Gemini response object:", response)

        if response and hasattr(response, "text") and response.text:
            print("âœ… Gemini returned a response.")
            return response.text.strip()
        else:
            print("âš ï¸ Gemini response was empty or malformed.")
            return "LLM returned an empty summary."

    except Exception as e:
        print("âŒ Error calling Gemini:", str(e))
        return f"LLM Error: {str(e)}"

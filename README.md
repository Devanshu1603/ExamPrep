# ğŸ“˜ AI Exam Preparation Chatbot

An intelligent, AI-powered assistant to help students prepare for exams by uploading books, PDFs, and YouTube links. It summarizes content, generates questions, and answers queries using a custom Retrieval-Augmented Generation (RAG) pipeline powered by Gemini and ChromaDB.

## ğŸš€ Features

- ğŸ“„ PDF/Book Upload & Summarization
- ğŸ“º YouTube Transcript Summarization
- â“ Question Generation (from books or videos)
- ğŸ’¬ RAG-based QA Chat Interface
- ğŸ§  Gemini 1.5 Flash LLM Integration
- ğŸ§  ChromaDB for Vector Search
- ğŸ” Firebase Auth (Login/Signup)
- ğŸŒ Built using FastAPI, React, Tailwind, Firebase

---

## ğŸ§© Project Structure

```
backend/
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ summarize.py         # Summarize syllabus/book
â”‚   â”œâ”€â”€ yt_summarize.py      # Summarize YouTube video
â”‚   â”œâ”€â”€ question_gen.py      # Generate questions from text
â”‚   â”œâ”€â”€ rag_query.py         # RAG query handler (QA using context)
â”‚   â””â”€â”€ upload.py            # Handles PDF upload, chunking, and storage
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ pdf_processor.py     # Extract text from PDFs using PyMuPDF
â”‚   â”œâ”€â”€ chunker.py           # Recursive chunking of extracted text
â”‚   â”œâ”€â”€ embedder.py          # Generate embeddings using Gemini API
â”‚   â”œâ”€â”€ vector_db.py         # Store & query chunks in ChromaDB
â”‚   â”œâ”€â”€ llm_wrapper.py       # Gemini API integration
â”‚   â””â”€â”€ yt_summarizer.py     # Fetch transcript and summarize YouTube videos
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ env.py               # Environment variables (API keys etc.)
â”‚   â””â”€â”€ firebase_admin.py    # Firebase Admin SDK for Firestore usage
â””â”€â”€ main.py                  # FastAPI app entrypoint
```

---

## ğŸ“¥ How It Works (Flow)

### Step-by-step Flow (PDF Upload to QA)

1. **User uploads PDF** â Frontend POSTs to `/upload/`
2. **`upload.py`** calls `pdf_processor.py` to extract text
3. **Text** â `chunker.py` â breaks into recursive chunks
4. **Chunks** â `embedder.py` â embeddings generated via Gemini
5. **Chunks + embeddings** â stored into `ChromaDB` via `vector_db.py`
6. **User asks question** â POSTs to `/query/` in `rag_query.py`
7. **Top chunks retrieved** â `vector_db.py`
8. **Prompt built with context** â `llm_wrapper.py` sends to Gemini
9. **Answer returned** â back to user

---

## ğŸ› ï¸ Tech Stack

### Backend
- âš¡ FastAPI (Python)
- ğŸ” ChromaDB (Vector DB)
- ğŸŒ Gemini API (via `google.generativeai`)
- ğŸ” Firebase Admin SDK (Firestore for chat history)

### Frontend (not included here)
- React + Tailwind + Firebase Auth

---

## ğŸ”‘ Setup Instructions

1. Clone the repo

```bash
git clone https://github.com/yourusername/ExamPrep.git
cd ExamPrep/backend
```

2. Create a `.env` file for backend

```env
GOOGLE_API_KEY=your_gemini_key
FIREBASE_PROJECT_ID=your_project_id
FIREBASE_PRIVATE_KEY="-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n"
FIREBASE_CLIENT_EMAIL=abc@yourproject.iam.gserviceaccount.com
```

3. Install dependencies

```bash
pip install -r requirements.txt
```

4. Run the FastAPI server

```bash
uvicorn main:app --reload
```

---

## ğŸ“¦ API Endpoints

| Route              | Method | Description |
|-------------------|--------|-------------|
| `/upload/`        | POST   | Upload PDF |
| `/summarize/`     | GET    | Summarize given text |
| `/yt-summarize/`  | GET    | Summarize YouTube transcript |
| `/question-gen/`  | POST   | Generate questions from content |
| `/query/`         | POST   | Ask a question (RAG pipeline) |

---

## ğŸ¤– Credits

Built with â¤ï¸ using FastAPI + Gemini + ChromaDB.
